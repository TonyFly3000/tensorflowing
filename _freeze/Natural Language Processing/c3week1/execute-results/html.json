{
  "hash": "79904b661aa0ea74817b9b322a3c0027",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"W1:Sentiment in text\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n# word based encodings,Using API to  make words into numbers.\n\n![](images/1.png){width=\"424\"}\n\n::: {#b3e8e98a .cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Define input sentences\nsentences = [\n    'i love my dog',\n    'I, love my cat'\n    ]\n\n# Initialize the Tokenizer class only encode max 100 words\ntokenizer = Tokenizer(num_words = 100)\n\n# Generate indices for each word in the corpus\ntokenizer.fit_on_texts(sentences)\n\n# Get the indices and print it (dictionary)\nword_index = tokenizer.word_index\nprint(word_index)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n```\n:::\n:::\n\n\n# add Out-of-vocabulary tokens\n\n::: {#cead7934 .cell execution_count=2}\n``` {.python .cell-code}\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Define your input texts\nsentences = [\n    'I love my dog',\n    'I love my cat',\n    'You love my dog!',\n    'Do you think my dog is amazing?'\n]\n\n# Initialize the Tokenizer class,\"<OOV>\" make all not seen word into a number.\ntokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n\n# Tokenize the input sentences\ntokenizer.fit_on_texts(sentences)\n\n# Get the word index dictionary\nword_index = tokenizer.word_index\n\n# Generate list of token sequences\nsequences = tokenizer.texts_to_sequences(sentences)\n\n# Print the result\nprint(\"\\nWord Index = \" , word_index)\nprint(\"\\nSequences = \" , sequences)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWord Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n\nSequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n```\n:::\n:::\n\n\n# padding\n\n::: {#be8374eb .cell execution_count=3}\n``` {.python .cell-code}\ntest_data = [\n    'i really love my dog',\n    'my dog loves my manatee'\n]\n\n# Generate the sequences\ntest_seq = tokenizer.texts_to_sequences(test_data)\n\n# Print the word index dictionary\nprint(\"\\nWord Index = \" , word_index)\n\n# Print the sequences with OOV\nprint(\"\\nTest Sequence = \", test_seq)\n\n\n# maxlen to 10 words\n#padded = pad_sequences(test_seq, maxlen=10)\n\n# make 0 at the end\npadded = pad_sequences(test_seq, padding='post')\nprint(\"\\nPadded Test Sequence: \")\nprint(padded)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWord Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n\nTest Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n\nPadded Test Sequence: \n[[5 1 3 2 4]\n [2 4 1 2 1]]\n```\n:::\n:::\n\n\n# download data\n\n::: {#c61e5939 .cell execution_count=4}\n``` {.python .cell-code}\nimport urllib.request\nurllib.request.urlretrieve(\"https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\",\"sarcasm.json\")\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n('sarcasm.json', <http.client.HTTPMessage at 0x167569c50>)\n```\n:::\n:::\n\n\n# read data\n\n::: {#c5dfc903 .cell execution_count=5}\n``` {.python .cell-code}\nimport json\n\n# Load the JSON file\nwith open(\"./sarcasm.json\", 'r') as f:\n    datastore = json.load(f)\n```\n:::\n\n\n::: {#c07ad358 .cell execution_count=6}\n``` {.python .cell-code}\n# Non-sarcastic headline\nprint(datastore[0])\n\n# Sarcastic headline\nprint(datastore[20000])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n{'article_link': 'https://www.theonion.com/pediatricians-announce-2011-newborns-are-ugliest-babies-1819572977', 'headline': 'pediatricians announce 2011 newborns are ugliest babies in 30 years', 'is_sarcastic': 1}\n```\n:::\n:::\n\n\n::: {#9979e5d0 .cell execution_count=7}\n``` {.python .cell-code}\n# Initialize lists\nsentences = [] \nlabels = []\nurls = []\n\n# Append elements in the dictionaries into each list\nfor item in datastore:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])\n```\n:::\n\n\n::: {#977f2f45 .cell execution_count=8}\n``` {.python .cell-code}\nprint(len(sentences))\n\nprint(len(labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n26709\n26709\n```\n:::\n:::\n\n\n::: {#4eb90f85 .cell execution_count=9}\n``` {.python .cell-code}\nsentences[2]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n\"mom starting to fear son's web series closest thing she will have to grandchild\"\n```\n:::\n:::\n\n\n::: {#4e7fbc19 .cell execution_count=10}\n``` {.python .cell-code}\nlabels[2]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n1\n```\n:::\n:::\n\n\n26709 sentences and 40 is the longest word in a  sentences\n\n::: {#7a1b39df .cell execution_count=11}\n``` {.python .cell-code}\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Initialize the Tokenizer class\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\n\n# Generate the word index dictionary\ntokenizer.fit_on_texts(sentences)\n\n# Print the length of the word index\nword_index = tokenizer.word_index\nprint(f'number of words in word_index: {len(word_index)}')\n\n# Print the word index\n#print(f'word_index: {word_index}')\nprint()\n\n# Generate and pad the sequences\nsequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding='post')\n\n# Print a sample headline\nindex = 2\nprint(f'sample headline: {sentences[index]}')\nprint()\n\nprint(f'padded sequence: {padded[index]}')\nprint()\n\n# Print dimensions of padded sequences\nprint(f'shape of padded sequences: {padded.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnumber of words in word_index: 29657\n\nsample headline: mom starting to fear son's web series closest thing she will have to grandchild\n\npadded sequence: [  145   838     2   907  1749  2093   582  4719   221   143    39    46\n     2 10736     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0]\n\nshape of padded sequences: (26709, 40)\n```\n:::\n:::\n\n\n# resource:\n\nhttps://www.coursera.org/learn/natural-language-processing-tensorflow\n\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C3\n\n",
    "supporting": [
      "c3week1_files"
    ],
    "filters": [],
    "includes": {}
  }
}