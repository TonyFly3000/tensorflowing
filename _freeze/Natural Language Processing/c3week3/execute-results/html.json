{
  "hash": "3d9f8b50af0bbc250eb989d8eab1d046",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"W3:Sequence models\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n# LSTMs\n\n## download data \n\nsubwords positive/negative IMDB movie review \n\n::: {#7b2a0125 .cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow_datasets as tfds\n\n# Download the subword encoded pretokenized dataset\ndataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n\n# Get the tokenizer\ntokenizer = info.features['text'].encoder\n```\n:::\n\n\n## make training and testing data\n\n::: {#0c41102a .cell execution_count=2}\n``` {.python .cell-code}\nBUFFER_SIZE = 10000\nBATCH_SIZE = 256\n\n# Get the train and test splits\ntrain_data, test_data = dataset['train'], dataset['test'], \n\n# Shuffle the training data\ntrain_dataset = train_data.shuffle(BUFFER_SIZE)\n\n# Batch and pad the datasets to the maximum length of the sequences\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE)\ntest_dataset = test_data.padded_batch(BATCH_SIZE)\n```\n:::\n\n\n## define model\n\n::: {#f9487059 .cell execution_count=3}\n``` {.python .cell-code}\nimport tensorflow as tf\n\n# Hyperparameters\nembedding_dim = 32\nlstm_dim = 32\ndense_dim = 16\n\n# Build the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim,return_sequence=True)),\n      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(dense_dim, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n```\n:::\n\n\n::: {#35bc08ec .cell execution_count=4}\n``` {.python .cell-code}\n# Print the model summary\nmodel.summary()\n```\n:::\n\n\n## compile model\n\n::: {#bfc7c406 .cell execution_count=5}\n``` {.python .cell-code}\n# Set the training parameters\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n```\n:::\n\n\n## training model\n\n::: {#6b57bf5d .cell execution_count=6}\n``` {.python .cell-code}\nNUM_EPOCHS = 2\n\nhistory = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)\n```\n:::\n\n\n## model result\n\n::: {#c4aac9dd .cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Plot utility\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\n# Plot the accuracy and results \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")\n```\n:::\n\n\n# resource:\n\nhttps://www.coursera.org/learn/natural-language-processing-tensorflow\n\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C3\n\n",
    "supporting": [
      "c3week3_files"
    ],
    "filters": [],
    "includes": {}
  }
}