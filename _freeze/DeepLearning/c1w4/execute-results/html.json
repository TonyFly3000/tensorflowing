{
  "hash": "b5faae442d9beb68b461d283c75de377",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to TensorFlow W4:Using Real-world Images\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\nby Laurence Moroney\n\n# download data\n\n## download horse-or-human.zip\n\n::: {#554eddcd .cell execution_count=1}\n``` {.python .cell-code}\nimport urllib.request\nurllib.request.urlretrieve(\"https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\", \"horse-or-human.zip\")\n```\n:::\n\n\n## download validation-horse-or-human.zip\n\n::: {#cc9a2fb6 .cell execution_count=2}\n``` {.python .cell-code}\nimport urllib.request\nurllib.request.urlretrieve(\"https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\", \"validation-horse-or-human.zip\")\n```\n:::\n\n\n::: {#c5aea7ed .cell execution_count=3}\n``` {.python .cell-code}\nimport zipfile\n#| eval: false\n# Unzip training set\nlocal_zip = './horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('./horse-or-human')\n\n# Unzip validation set\nlocal_zip = './validation-horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('./validation-horse-or-human')\n\nzip_ref.close()\n```\n:::\n\n\n::: {#723dd215 .cell execution_count=4}\n``` {.python .cell-code}\nfrom PIL import Image\n\nim = Image.open('./horse-or-human/horses/horse01-0.png')\nim.size # (width,height) \n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(300, 300)\n```\n:::\n:::\n\n\n::: {#3f6b1f10 .cell execution_count=5}\n``` {.python .cell-code}\nimport os\n\n# Directory with training horse pictures\ntrain_horse_dir = os.path.join('./horse-or-human/horses')\n\n# Directory with training human pictures\ntrain_human_dir = os.path.join('./horse-or-human/humans')\n\n# Directory with validation horse pictures\nvalidation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n\n# Directory with validation human pictures\nvalidation_human_dir = os.path.join('./validation-horse-or-human/humans')\n```\n:::\n\n\n::: {#a3672bb6 .cell execution_count=6}\n``` {.python .cell-code}\ntrain_horse_names = os.listdir(train_horse_dir)\nprint(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n\ntrain_human_names = os.listdir(train_human_dir)\nprint(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n\nvalidation_horse_names = os.listdir(validation_horse_dir)\nprint(f'VAL SET HORSES: {validation_horse_names[:10]}')\n\nvalidation_human_names = os.listdir(validation_human_dir)\nprint(f'VAL SET HUMANS: {validation_human_names[:10]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTRAIN SET HORSES: ['horse43-5.png', 'horse06-5.png', 'horse20-6.png', 'horse04-7.png', 'horse41-7.png', 'horse22-4.png', 'horse19-2.png', 'horse24-2.png', 'horse37-8.png', 'horse02-1.png']\nTRAIN SET HUMANS: ['human17-22.png', 'human10-17.png', 'human10-03.png', 'human07-27.png', 'human09-22.png', 'human05-22.png', 'human02-03.png', 'human02-17.png', 'human15-27.png', 'human12-12.png']\nVAL SET HORSES: ['horse1-204.png', 'horse2-112.png', 'horse3-498.png', 'horse5-032.png', 'horse5-018.png', 'horse1-170.png', 'horse5-192.png', 'horse1-411.png', 'horse4-232.png', 'horse3-070.png']\nVAL SET HUMANS: ['valhuman04-20.png', 'valhuman03-01.png', 'valhuman04-08.png', 'valhuman03-15.png', 'valhuman01-04.png', 'valhuman01-10.png', 'valhuman01-11.png', 'valhuman01-05.png', 'valhuman03-14.png', 'valhuman03-00.png']\n```\n:::\n:::\n\n\n::: {#03d17dbb .cell execution_count=7}\n``` {.python .cell-code}\nprint(f'total training horse images: {len(os.listdir(train_horse_dir))}')\nprint(f'total training human images: {len(os.listdir(train_human_dir))}')\nprint(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\nprint(f'total validation human images: {len(os.listdir(validation_human_dir))}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntotal training horse images: 500\ntotal training human images: 527\ntotal validation horse images: 128\ntotal validation human images: 128\n```\n:::\n:::\n\n\n::: {#733e0966 .cell execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0\n\n# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_horse_pix = [os.path.join(train_horse_dir, fname) \n                for fname in train_horse_names[pic_index-8:pic_index]]\nnext_human_pix = [os.path.join(train_human_dir, fname) \n                for fname in train_human_names[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_horse_pix+next_human_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](c1w4_files/figure-html/cell-9-output-1.png){width=1208 height=1202}\n:::\n:::\n\n\n::: {#38785314 .cell execution_count=9}\n``` {.python .cell-code}\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nimport os\nprint(tf.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.16.1\n```\n:::\n:::\n\n\n# Load the data\n\npicture file structure\n\n![](images/2.png){width=\"422\"}\n\nImageDataGenerator: All images will be resized to 300x300\n\n::: {#bbbf7c4a .cell execution_count=10}\n``` {.python .cell-code}\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1./255\ntrain_datagen = ImageDataGenerator(rescale=1/255)\nvalidation_datagen = ImageDataGenerator(rescale=1/255)\n\n# Flow training images in batches of 128 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        './horse-or-human/',  # This is the source directory for training images\n        target_size=(300, 300),  # All images will be resized to 300x300\n        batch_size=128,\n        # Since you use binary_crossentropy loss, you need binary labels\n        class_mode='binary')\n\n# Flow validation images in batches of 128 using validation_datagen generator\nvalidation_generator = validation_datagen.flow_from_directory(\n        './validation-horse-or-human/',  # This is the source directory for validation images\n        target_size=(300, 300),  # All images will be resized to 300x300\n        batch_size=32,\n        # Since you use binary_crossentropy loss, you need binary labels\n        class_mode='binary')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 1027 images belonging to 2 classes.\nFound 256 images belonging to 2 classes.\n```\n:::\n:::\n\n\n# define convolutional model\n\ninput 300 by 300 color image\n\nflow into 16 3by3 convolutional layers and 2by2 pooling\n\noutput is 1 neural(0/1) sigmoid function.its good for binary\n\n::: {#b84a323c .cell execution_count=11}\n``` {.python .cell-code}\nimport tensorflow as tf\n\nmodel = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fifth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n```\n:::\n\n\n::: {#c5459f65 .cell execution_count=12}\n``` {.python .cell-code}\n# Print the model summary\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,097</span> (6.50 MB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,097</span> (6.50 MB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n# compile model\n\n::: {#465a8d22 .cell execution_count=13}\n``` {.python .cell-code}\n# v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(learning_rate=0.001),\n              metrics=['accuracy'])\n```\n:::\n\n\n# Callbacks\n\n::: {#623401d1 .cell execution_count=14}\n``` {.python .cell-code}\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    '''\n    Halts the training when the loss falls below 0.15\n\n    Args:\n      epoch (integer) - index of epoch (required but unused in the function definition below)\n      logs (dict) - metric results from the training epoch\n    '''\n\n    # Check the loss\n    if(logs.get('loss') < 0.15):\n\n      # Stop if threshold is met\n      print(\"\\nLoss is lower than 0.2 so cancelling training!\")\n      print(\"cancelling training with:\")\n      print(epoch+1)\n      self.model.stop_training = True\n\n# Instantiate class\ncallbacks = myCallback()\n```\n:::\n\n\n# train model\n\n::: {#7f60debf .cell execution_count=15}\n``` {.python .cell-code}\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=8,  \n      epochs=10,\n      verbose=0,\n      validation_data = validation_generator,\n      validation_steps=8,\n      callbacks=[callbacks])\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoss is lower than 0.2 so cancelling training!\ncancelling training with:\n10\n```\n:::\n:::\n\n\n# Prediction\n\nPrediction a validation horses\n\n::: {#962300ca .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\n\nfrom tensorflow.keras.utils import load_img, img_to_array\n\nfn='horse1-241.png'\npath = './validation-horse-or-human/horses/'+ fn\nimg = load_img(path, target_size=(300, 300))\nx = img_to_array(img)\nx /= 255\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\nclasses = model.predict(images, batch_size=10)\nprint(classes[0])\nif classes[0]>0.5:\n  print(fn+\" is a human\")\nelse:\n  print(fn+\" is a horse\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step\n[5.7587976e-11]\nhorse1-241.png is a horse\n```\n:::\n:::\n\n\nPrediction a human from internet\n\n::: {#ab01d3f7 .cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\n\nfrom tensorflow.keras.utils import load_img, img_to_array\n\nfn='daniel craig.png'\npath = './random/'+ fn\nimg = load_img(path, target_size=(300, 300))\nx = img_to_array(img)\nx /= 255\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\nclasses = model.predict(images, batch_size=10)\nprint(classes[0])\nif classes[0]>0.5:\n  print(fn+\" is a human\")\nelse:\n  print(fn+\" is a horse\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\n[0.01623141]\ndaniel craig.png is a horse\n```\n:::\n:::\n\n\n# resource:\n\nhttps://www.coursera.org/learn/introduction-tensorflow/home/info\n\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C1\n\nhttps://github.com/zalandoresearch/fashion-mnist\n\n",
    "supporting": [
      "c1w4_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}