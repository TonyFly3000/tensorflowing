{
  "hash": "9d19de6b58cf508ee1295cba2d96c9e4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"W2:Deep Neural Networks for Time Series\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\nWeek 2 Deep Neural Networks for Time Series\n\nHaving explored time series and some of the common attributes of time series such as trend and seasonality, and then having used statistical methods for projection, let's now begin to teach neural networks to recognize and predict on time series!\n\n::: {#0a014e66 .cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\n```\n:::\n\n\n# Create a Simple Dataset\n\n::: {#e7dd56fa .cell execution_count=2}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Preview the result\nfor val in dataset:\n   print(val)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n```\n:::\n:::\n\n\n# Windowing the data\n\n::: {#8c5eddb3 .cell execution_count=3}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Window the data but only take those with the specified size\ndataset = dataset.window(size=5, shift=1, drop_remainder=True)\n```\n:::\n\n\n::: {#9f0b5bb6 .cell execution_count=4}\n``` {.python .cell-code}\n# Print the result\nfor window_dataset in dataset:\n  print([item.numpy() for item in window_dataset])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0, 1, 2, 3, 4]\n[1, 2, 3, 4, 5]\n[2, 3, 4, 5, 6]\n[3, 4, 5, 6, 7]\n[4, 5, 6, 7, 8]\n[5, 6, 7, 8, 9]\n```\n:::\n:::\n\n\n# Flatten the Windows\n\n::: {#a0019b97 .cell execution_count=5}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Window the data but only take those with the specified size\ndataset = dataset.window(5, shift=1, drop_remainder=True)\n\n# Flatten the windows by putting its elements in a single batch\ndataset = dataset.flat_map(lambda window: window.batch(5))\n\n# Print the results\nfor window in dataset:\n  print(window.numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 1 2 3 4]\n[1 2 3 4 5]\n[2 3 4 5 6]\n[3 4 5 6 7]\n[4 5 6 7 8]\n[5 6 7 8 9]\n```\n:::\n:::\n\n\n# Group into features and labels\n\n::: {#d384ab52 .cell execution_count=6}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Window the data but only take those with the specified size\ndataset = dataset.window(5, shift=1, drop_remainder=True)\n\n# Flatten the windows by putting its elements in a single batch\ndataset = dataset.flat_map(lambda window: window.batch(5))\n\n# Create tuples with features (first four elements of the window) and labels (last element)\ndataset = dataset.map(lambda window: (window[:-1], window[-1]))\n\n# Print the results\nfor x,y in dataset:\n  print(\"x = \", x.numpy())\n  print(\"y = \", y.numpy())\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx =  [0 1 2 3]\ny =  4\n\nx =  [1 2 3 4]\ny =  5\n\nx =  [2 3 4 5]\ny =  6\n\nx =  [3 4 5 6]\ny =  7\n\nx =  [4 5 6 7]\ny =  8\n\nx =  [5 6 7 8]\ny =  9\n\n```\n:::\n:::\n\n\n# Shuffle the data\n\n::: {#18d66a59 .cell execution_count=7}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Window the data but only take those with the specified size\ndataset = dataset.window(5, shift=1, drop_remainder=True)\n\n# Flatten the windows by putting its elements in a single batch\ndataset = dataset.flat_map(lambda window: window.batch(5))\n\n# Create tuples with features (first four elements of the window) and labels (last element)\ndataset = dataset.map(lambda window: (window[:-1], window[-1]))\n\n# Shuffle the windows\ndataset = dataset.shuffle(buffer_size=10)\n\n# Print the results\nfor x,y in dataset:\n  print(\"x = \", x.numpy())\n  print(\"y = \", y.numpy())\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx =  [1 2 3 4]\ny =  5\n\nx =  [4 5 6 7]\ny =  8\n\nx =  [0 1 2 3]\ny =  4\n\nx =  [3 4 5 6]\ny =  7\n\nx =  [2 3 4 5]\ny =  6\n\nx =  [5 6 7 8]\ny =  9\n\n```\n:::\n:::\n\n\n# Create batches for training\n\n::: {#31c04919 .cell execution_count=8}\n``` {.python .cell-code}\n# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\ndataset = tf.data.Dataset.range(10)\n\n# Window the data but only take those with the specified size\ndataset = dataset.window(5, shift=1, drop_remainder=True)\n\n# Flatten the windows by putting its elements in a single batch\ndataset = dataset.flat_map(lambda window: window.batch(5))\n\n# Create tuples with features (first four elements of the window) and labels (last element)\ndataset = dataset.map(lambda window: (window[:-1], window[-1]))\n\n# Shuffle the windows\ndataset = dataset.shuffle(buffer_size=10)\n\n# Create batches of windows\ndataset = dataset.batch(2).prefetch(1)\n\n# Print the results\nfor x,y in dataset:\n  print(\"x = \", x.numpy())\n  print(\"y = \", y.numpy())\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx =  [[0 1 2 3]\n [5 6 7 8]]\ny =  [4 9]\n\nx =  [[3 4 5 6]\n [1 2 3 4]]\ny =  [7 5]\n\nx =  [[2 3 4 5]\n [4 5 6 7]]\ny =  [6 8]\n\n```\n:::\n:::\n\n\n# Utilities\n\n::: {#2e7b1bed .cell execution_count=9}\n``` {.python .cell-code}\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#adccadce .cell execution_count=10}\n``` {.python .cell-code}\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    \"\"\"\n    Visualizes time series data\n\n    Args:\n      time (array of int) - contains the time steps\n      series (array of int) - contains the measurements for each time step\n      format - line style when plotting the graph\n      label - tag for the line\n      start - first time step to plot\n      end - last time step to plot\n    \"\"\"\n\n    # Setup dimensions of the graph figure\n    plt.figure(figsize=(10, 6))\n    \n    if type(series) is tuple:\n\n      for series_num in series:\n        # Plot the time series data\n        plt.plot(time[start:end], series_num[start:end], format)\n\n    else:\n      # Plot the time series data\n      plt.plot(time[start:end], series[start:end], format)\n\n    # Label the x-axis\n    plt.xlabel(\"Time\")\n\n    # Label the y-axis\n    plt.ylabel(\"Value\")\n\n    # Overlay a grid on the graph\n    plt.grid(True)\n\n    # Draw the graph on screen\n    plt.show()\n\n\ndef trend(time, slope=0):\n    \"\"\"\n    Generates synthetic data that follows a straight line given a slope value.\n\n    Args:\n      time (array of int) - contains the time steps\n      slope (float) - determines the direction and steepness of the line\n\n    Returns:\n      series (array of float) - measurements that follow a straight line\n    \"\"\"\n\n    # Compute the linear series given the slope\n    series = slope * time\n\n    return series\n\n\ndef seasonal_pattern(season_time):\n    \"\"\"\n    Just an arbitrary pattern, you can change it if you wish\n    \n    Args:\n      season_time (array of float) - contains the measurements per time step\n\n    Returns:\n      data_pattern (array of float) -  contains revised measurement values according \n                                  to the defined pattern\n    \"\"\"\n\n    # Generate the values using an arbitrary pattern\n    data_pattern = np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 / np.exp(3 * season_time))\n    \n    return data_pattern\n\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"\n    Repeats the same pattern at each period\n\n    Args:\n      time (array of int) - contains the time steps\n      period (int) - number of time steps before the pattern repeats\n      amplitude (int) - peak measured value in a period\n      phase (int) - number of time steps to shift the measured values\n\n    Returns:\n      data_pattern (array of float) - seasonal data scaled by the defined amplitude\n    \"\"\"\n    \n    # Define the measured values per period\n    season_time = ((time + phase) % period) / period\n\n    # Generates the seasonal data scaled by the defined amplitude\n    data_pattern = amplitude * seasonal_pattern(season_time)\n\n    return data_pattern\n\n\ndef noise(time, noise_level=1, seed=None):\n    \"\"\"Generates a normally distributed noisy signal\n\n    Args:\n      time (array of int) - contains the time steps\n      noise_level (float) - scaling factor for the generated signal\n      seed (int) - number generator seed for repeatability\n\n    Returns:\n      noise (array of float) - the noisy signal\n    \"\"\"\n\n    # Initialize the random number generator\n    rnd = np.random.RandomState(seed)\n\n    # Generate a random number for each time step and scale by the noise level\n    noise = rnd.randn(len(time)) * noise_level\n    \n    return noise\n```\n:::\n\n\n# Generate the Synthetic Data\n\n::: {#9fcb9c33 .cell execution_count=11}\n``` {.python .cell-code}\n# Parameters\ntime = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\n# Plot the results\nplot_series(time, series)\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-12-output-1.png){width=819 height=503}\n:::\n:::\n\n\n# Split the Dataset\n\n::: {#e8cbf89c .cell execution_count=12}\n``` {.python .cell-code}\n# Define the split time\nsplit_time = 1000\n\n# Get the train set \ntime_train = time[:split_time]\nx_train = series[:split_time]\n\n# Get the validation set\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n```\n:::\n\n\n::: {#bacb0816 .cell execution_count=13}\n``` {.python .cell-code}\n# Plot the train set\nplot_series(time_train, x_train)\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-14-output-1.png){width=819 height=503}\n:::\n:::\n\n\n::: {#e9d7f196 .cell execution_count=14}\n``` {.python .cell-code}\n# Plot the validation set\nplot_series(time_valid, x_valid)\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-15-output-1.png){width=816 height=503}\n:::\n:::\n\n\n# Prepare features and labels\n\nusing 20 day window\n\n::: {#9fc42961 .cell execution_count=15}\n``` {.python .cell-code}\n# Parameters\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000\n```\n:::\n\n\n::: {#bc7f58bc .cell execution_count=16}\n``` {.python .cell-code}\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"Generates dataset windows\n\n    Args:\n      series (array of float) - contains the values of the time series\n      window_size (int) - the number of time steps to include in the feature\n      batch_size (int) - the batch size\n      shuffle_buffer(int) - buffer size to use for the shuffle method\n\n    Returns:\n      dataset (TF Dataset) - TF Dataset containing time windows\n    \"\"\"\n  \n    # Generate a TF Dataset from the series values\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    \n    # Window the data but only take those with the specified size\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    \n    # Flatten the windows by putting its elements in a single batch\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n\n    # Create tuples with features and labels \n    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n\n    # Shuffle the windows\n    dataset = dataset.shuffle(shuffle_buffer)\n    \n    # Create batches of windows\n    dataset = dataset.batch(batch_size).prefetch(1)\n    \n    return dataset\n```\n:::\n\n\n::: {#300f17b8 .cell execution_count=17}\n``` {.python .cell-code}\n# Generate the dataset windows\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n```\n:::\n\n\n::: {#2361528a .cell execution_count=18}\n``` {.python .cell-code}\n# Print properties of a single batch\nfor windows in dataset.take(1):\n  print(f'data type: {type(windows)}')\n  print(f'number of elements in the tuple: {len(windows)}')\n  print(f'shape of first element: {windows[0].shape}')\n  print(f'shape of second element: {windows[1].shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndata type: <class 'tuple'>\nnumber of elements in the tuple: 2\nshape of first element: (32, 20)\nshape of second element: (32,)\n```\n:::\n:::\n\n\n# define single layer neural model\n\n::: {#e2b42e89 .cell execution_count=19}\n``` {.python .cell-code}\n# Build the single layer neural network\nl0 = tf.keras.layers.Dense(1, input_shape=[window_size])\nmodel = tf.keras.models.Sequential([l0])\n\n# Print the initial layer weights\nprint(\"Layer weights: \\n {} \\n\".format(l0.get_weights()))\n\n# Print the model summary\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLayer weights: \n [array([[-0.02897543],\n       [ 0.2368316 ],\n       [-0.4459325 ],\n       [-0.00241488],\n       [ 0.48065752],\n       [-0.48599264],\n       [ 0.51217467],\n       [-0.01346123],\n       [ 0.09258026],\n       [ 0.14586818],\n       [ 0.3576712 ],\n       [-0.5222965 ],\n       [ 0.28616023],\n       [ 0.14155024],\n       [ 0.09476584],\n       [ 0.2801147 ],\n       [ 0.07839262],\n       [-0.22657192],\n       [ 0.10993838],\n       [ 0.1691292 ]], dtype=float32), array([0.], dtype=float32)] \n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> (84.00 B)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> (84.00 B)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n# compile  model\n\n::: {#9ecd205c .cell execution_count=20}\n``` {.python .cell-code}\n# Set the training parameters\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))\n```\n:::\n\n\n# Train the Model\n\n::: {#873f9eca .cell execution_count=21}\n``` {.python .cell-code}\n# Train the model\nmodel.fit(dataset,epochs=100,verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n<keras.src.callbacks.history.History at 0x14d7921d0>\n```\n:::\n:::\n\n\n::: {#fe95fccb .cell execution_count=22}\n``` {.python .cell-code}\n# Print the layer weights\nprint(\"Layer weights {}\".format(l0.get_weights()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLayer weights [array([[-0.02693498],\n       [ 0.02551215],\n       [-0.03809907],\n       [-0.02330507],\n       [ 0.1171276 ],\n       [-0.10470365],\n       [ 0.07021185],\n       [-0.03254524],\n       [-0.00710663],\n       [ 0.01842318],\n       [ 0.057825  ],\n       [-0.12609479],\n       [ 0.014718  ],\n       [ 0.03457457],\n       [ 0.0259654 ],\n       [ 0.07261312],\n       [ 0.04803773],\n       [ 0.10154756],\n       [ 0.30292708],\n       [ 0.44251084]], dtype=float32), array([0.01477173], dtype=float32)]\n```\n:::\n:::\n\n\n# Model Prediction\n\n::: {#03b3b9dc .cell execution_count=23}\n``` {.python .cell-code}\n# Shape of the first 20 data points slice\nprint(f'shape of series[0:20]: {series[0:20].shape}')\n\n# Shape after adding a batch dimension\nprint(f'shape of series[0:20][np.newaxis]: {series[0:20][np.newaxis].shape}')\n\n# Shape after adding a batch dimension (alternate way)\nprint(f'shape of series[0:20][np.newaxis]: {np.expand_dims(series[0:20], axis=0).shape}')\n\n# Sample model prediction\nprint(f'model prediction: {model.predict(series[0:20][np.newaxis])}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape of series[0:20]: (20,)\nshape of series[0:20][np.newaxis]: (1, 20)\nshape of series[0:20][np.newaxis]: (1, 20)\n\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step\nmodel prediction: [[42.610508]]\n```\n:::\n:::\n\n\n::: {#26082650 .cell execution_count=24}\n``` {.python .cell-code}\n# Initialize a list\nforecast = []\n\n# Use the model to predict data points per window size\nfor time in range(len(series) - window_size):\n  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\n# Slice the points that are aligned with the validation set\nforecast = forecast[split_time - window_size:]\n```\n:::\n\n\n::: {#ed391534 .cell execution_count=25}\n``` {.python .cell-code}\n# Compare number of elements in the predictions and the validation set\nprint(f'length of the forecast list: {len(forecast)}')\nprint(f'shape of the validation set: {x_valid.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlength of the forecast list: 461\nshape of the validation set: (461,)\n```\n:::\n:::\n\n\n::: {#663753f1 .cell execution_count=26}\n``` {.python .cell-code}\n# Preview shapes after using the conversion and squeeze methods\nprint(f'shape after converting to numpy array: {np.array(forecast).shape}')\nprint(f'shape after squeezing: {np.array(forecast).squeeze().shape}')\n\n# Convert to a numpy array and drop single dimensional axes\nresults = np.array(forecast).squeeze()\n\n# Overlay the results with the validation set\nplot_series(time_valid, (x_valid, results))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape after converting to numpy array: (461, 1, 1)\nshape after squeezing: (461,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-27-output-2.png){width=816 height=503}\n:::\n:::\n\n\n::: {#3496ac7a .cell execution_count=27}\n``` {.python .cell-code}\n# Compute the metrics\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n50.470028\n```\n:::\n:::\n\n\n::: {#9ee4195c .cell execution_count=28}\n``` {.python .cell-code}\n# Compute the metrics\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5.173313\n```\n:::\n:::\n\n\n# DNN\n\n::: {#dd3dd211 .cell execution_count=29}\n``` {.python .cell-code}\n# Generate the dataset windows\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n```\n:::\n\n\n# # define DNN model\n\n::: {#386bd014 .cell execution_count=30}\n``` {.python .cell-code}\n# Build the model\nmodel_baseline = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n    tf.keras.layers.Dense(10, activation=\"relu\"), \n    tf.keras.layers.Dense(1)\n])\n\n# Print the model summary\nmodel_baseline.summary()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331</span> (1.29 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331</span> (1.29 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n# compile model\n\n::: {#23a733ba .cell execution_count=31}\n``` {.python .cell-code}\n# Set the training parameters\nmodel_baseline.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))\n```\n:::\n\n\n# Train the Model\n\n::: {#f32557b7 .cell execution_count=32}\n``` {.python .cell-code}\n# Train the model\nmodel_baseline.fit(dataset,epochs=100,verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n<keras.src.callbacks.history.History at 0x177b2df90>\n```\n:::\n:::\n\n\n# result\n\n::: {#910418f4 .cell execution_count=33}\n``` {.python .cell-code}\n# Initialize a list\nforecast = []\n\n# Reduce the original series\nforecast_series = series[split_time - window_size:]\n\n# Use the model to predict data points per window size\nfor time in range(len(forecast_series) - window_size):\n  forecast.append(model_baseline.predict(forecast_series[time:time + window_size][np.newaxis]))\n\n# Convert to a numpy array and drop single dimensional axes\nresults = np.array(forecast).squeeze()\n\n# Plot the results\nplot_series(time_valid, (x_valid, results))\n```\n:::\n\n\n::: {#c70d2584 .cell execution_count=34}\n``` {.python .cell-code}\n# Compute the metrics\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n46.421417\n```\n:::\n:::\n\n\n::: {#b105e801 .cell execution_count=35}\n``` {.python .cell-code}\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5.0056305\n```\n:::\n:::\n\n\n# Tune the learning rate\n\nclear\n\n::: {#58387b83 .cell execution_count=36}\n``` {.python .cell-code}\ntf.keras.backend.clear_session()\n```\n:::\n\n\n# define DNN model\n\n::: {#e9a5e504 .cell execution_count=37}\n``` {.python .cell-code}\n# Build the Model\nmodel_tune = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n    tf.keras.layers.Dense(10, activation=\"relu\"), \n    tf.keras.layers.Dense(1)\n])\n```\n:::\n\n\n# set callbacks for learning rate tunning\n\n::: {#3eb1c942 .cell execution_count=38}\n``` {.python .cell-code}\n# Set the learning rate scheduler\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20))\n```\n:::\n\n\n# complie model\n\n::: {#bd2b2e94 .cell execution_count=39}\n``` {.python .cell-code}\n# Initialize the optimizer\noptimizer = tf.keras.optimizers.SGD(momentum=0.9)\n\n# Set the training parameters\nmodel_tune.compile(loss=\"mse\", optimizer=optimizer)\n```\n:::\n\n\n# train model\n\n::: {#1d044ac5 .cell execution_count=40}\n``` {.python .cell-code}\n# Train the model\nhistory = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule],verbose=0)\n```\n:::\n\n\n::: {#cbcd8298 .cell execution_count=41}\n``` {.python .cell-code}\n# Define the learning rate array\nlrs = 1e-8 * (10 ** (np.arange(100) / 20))\n\n# Set the figure size\nplt.figure(figsize=(10, 6))\n\n# Set the grid\nplt.grid(True)\n\n# Plot the loss in log scale\nplt.semilogx(lrs, history.history[\"loss\"])\n\n# Increase the tickmarks size\nplt.tick_params('both', length=10, width=1, which='both')\n\n# Set the plot boundaries\nplt.axis([1e-8, 1e-3, 0, 300])\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-42-output-1.png){width=822 height=500}\n:::\n:::\n\n\n::: {#201f137f .cell execution_count=42}\n``` {.python .cell-code}\n# Build the model\nmodel_tune = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(10, activation=\"relu\", input_shape=[window_size]),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1)\n])\n```\n:::\n\n\n::: {#1fa80f21 .cell execution_count=43}\n``` {.python .cell-code}\n# Set the optimizer with the tuned learning rate\noptimizer = tf.keras.optimizers.SGD(learning_rate=4e-6, momentum=0.9)\n```\n:::\n\n\n::: {#64f35bcc .cell execution_count=44}\n``` {.python .cell-code}\n# Set the training parameters\nmodel_tune.compile(loss=\"mse\", optimizer=optimizer)\n\n# Train the model\nhistory = model_tune.fit(dataset, epochs=300,verbose=0)\n```\n:::\n\n\n::: {#61a3e665 .cell execution_count=45}\n``` {.python .cell-code}\n# Plot all\nloss = history.history['loss']\nepochs = range(len(loss))\nplot_loss = loss\nplt.plot(epochs, plot_loss, 'b', label='Training Loss')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-46-output-1.png){width=583 height=411}\n:::\n:::\n\n\n::: {#799826f2 .cell execution_count=46}\n``` {.python .cell-code}\n# Plot all excluding first 10\nloss = history.history['loss']\nepochs = range(10, len(loss))\nplot_loss = loss[10:]\nplt.plot(epochs, plot_loss, 'b', label='Training Loss')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](c4week2_files/figure-html/cell-47-output-1.png){width=579 height=413}\n:::\n:::\n\n\n::: {#ad74f67d .cell execution_count=47}\n``` {.python .cell-code}\n# Initialize a list\nforecast = []\n\n# Reduce the original series\nforecast_series = series[split_time - window_size:]\n\n# Use the model to predict data points per window size\nfor time in range(len(forecast_series) - window_size):\n  forecast.append(model_tune.predict(forecast_series[time:time + window_size][np.newaxis]))\n\n# Convert to a numpy array and drop single dimensional axes\nresults = np.array(forecast).squeeze()\n\n# Plot the results\nplot_series(time_valid, (x_valid, results))\n```\n:::\n\n\n::: {#448dbb1f .cell execution_count=48}\n``` {.python .cell-code}\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n42.02161\n```\n:::\n:::\n\n\n::: {#2d438e2c .cell execution_count=49}\n``` {.python .cell-code}\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.852905\n```\n:::\n:::\n\n\n# resource:\n\nhttps://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction\n\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C4\n\n",
    "supporting": [
      "c4week2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}