---
title: "W3:Enhancing Vision with Convolutional Neural Networks"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

Week3 Enhancing Vision with Convolutional Neural Networks

Welcome to week 3! In week 2 you saw a basic Neural Network for Computer Vision. It did the job nicely, but it was a little naive in its approach. This week weâ€™ll see how to make it better, as discussed by Laurence and Andrew here.


# convolutional

![](images/131.png)

![](images/132.png)

pooling

![](images/133.png)

```{python}
import tensorflow as tf
import numpy as np
from tensorflow import keras
import os
print(tf.__version__)
```

# Load the data

```{python}

# Load the Fashion MNIST dataset
fmnist = tf.keras.datasets.fashion_mnist

```

```{python}
# Load the training and test split of the Fashion MNIST dataset
(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

# You can put between 0 to 59999 here
index = 0

# Set number of characters per row when printing
np.set_printoptions(linewidth=320)

# Print the label and image
print(f'LABEL: {training_labels[index]}')
print(f'\nIMAGE PIXEL ARRAY:\n {training_images[index]}')
```

```{python}
# Visualize the image
plt.imshow(training_images[index])
```

You'll notice that all of the values in the number are between 0 and 255. If you are training a neural network especially in image processing, for various reasons it will usually learn better if you scale all values to between 0 and 1. It's a process called normalization and fortunately in Python, it's easy to normalize an array without looping. You do it like this:

```{python}
# Normalize the pixel values of the train and test images
training_images  = training_images / 255.0
test_images = test_images / 255.0
```

# define convolutional model

input 28 by 28 black&white image 

flow into32 3by3 convolutional layers and 2by2 pooling

output is 10 neural(0-9) with softmax function 

```{python}
# Define the model
model = tf.keras.models.Sequential([
                                                         
  # Add convolutions and max pooling
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2,2),

  # Add the same layers as before
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
```


```{python}
# Print the model summary
model.summary()
```

# compile model

```{python}
# the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs
model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

# Callbacks

```{python}
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    '''
    Halts the training when the loss falls below 0.4

    Args:
      epoch (integer) - index of epoch (required but unused in the function definition below)
      logs (dict) - metric results from the training epoch
    '''

    # Check the loss
    if(logs.get('loss') < 0.3):

      # Stop if threshold is met
      print("\nLoss is lower than 0.4 so cancelling training!")
      print("cancelling training with:")
      print(epoch+1)
      self.model.stop_training = True

# Instantiate class
callbacks = myCallback()
```

# train model

```{python}
model.fit(training_images, training_labels, epochs=10,callbacks=[callbacks])
```

# evaluate

```{python}
# Evaluate the model on unseen data
model.evaluate(test_images, test_labels)
```

# resource:

https://www.coursera.org/learn/introduction-tensorflow/home/info

https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C1

https://github.com/zalandoresearch/fashion-mnist
